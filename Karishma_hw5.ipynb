{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1.5: Scraping Review üèãÔ∏è‚ÄçüèãÔ∏è‚ÄçüèãÔ∏è‚Äç\n",
    "\n",
    "For this assignment, you will be scraping an API and a live website.\n",
    "\n",
    "### Table of Contents\n",
    "1. CFPB API\n",
    "2. Microworkers\n",
    "\n",
    "## Prelude: Importing Your Libraries \n",
    "The *first* first thing we're going to do is make sure we're all set up and ready to go. \n",
    "\n",
    "That means importing some libraries! I've got a cell below all ready for you to put in some libraries.\n",
    "\n",
    "Remember with third-party libraries, you will need to make sure they are actually installed before they will run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import native libraries\n",
    "import re\n",
    "import csv \n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import third party libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Scraping the CFPB\n",
    "\n",
    "The Consumer Finance and Protection Bureau was founded in the aftermath of the 2008 financial crisis. One of the things that they do is collect complaints from consumers about bad banks, lenders and other financial insitutions. This complaint data is available to the public in many forms. While you can download a big, horrible CSV file with _all_ of the data, it's usually easier (for you and your computer's memory storage) to use the API to only get the data you need.\n",
    "\n",
    "The CFPB uses [Socrata](https://www.tylertech.com/products/socrata) to manage their API, which is a company that helps a lot of public agencies share their data with the rest of the world. The way they have you request for data is kind of funky, but we will perservere together!\n",
    "\n",
    "[The homepage for the Consumer Complaint Database](https://cfpb.github.io/api/ccdb/index.html)<br>[API Reference](https://dev.socrata.com/foundry/data.consumerfinance.gov/s6ew-h6mp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Doing a single request\n",
    "\n",
    "#### Open the page as a JSON object with Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the first item in the returned list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are a lot of fields!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting a lil bit more specific\n",
    "\n",
    "When we get complaint data from just the endpoint, we are getting ALL the data‚Äìit's basically a firehose! However, we don't actually want all the complaints submitted to the CFPB! We only want specific kinds! \n",
    "\n",
    "In fact, we only want complaints that fit this criteria:\n",
    "- The consumer is based the state of New York\n",
    "- It was received by the CFPB between January 1, 2018 and January 1, 2019\n",
    "- It is about the product \"Debt collection\" and the sub-product \"Mortgage debt\"\n",
    "\n",
    "Using the `cfpb_endpoint`, we will build a url that requests just these kinds of complaints!\n",
    "\n",
    "We will first filter by each thing, and then write a url that filters all three at the same time! Woah!\n",
    "\n",
    "#### Filtering by state\n",
    "\n",
    "Look back at the piece of data we printed in Step 1. How can you tell which state the complaint is from? How are they formatting the state names‚Äìis it the full name, or an abbreviation of sort? Consider checking out the [API documentation](https://dev.socrata.com/foundry/data.consumerfinance.gov/s6ew-h6mp)'s \"Fields\" section if you're feeling a little lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering by date range\n",
    "\n",
    "Read the [between...and...](https://dev.socrata.com/docs/functions/between.html) page in the API documentation. This will explain how to query for complaints within a particular timeframe! Now use that knowledge to call all the complaints between January 1, 2018 and January 1, 2019!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering by sub-product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together\n",
    "\n",
    "Now that you've gotten data from each *individual* filter, let's combine them! You can use multiple filters by sticking an `&` between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gutcheck:** Count how many items you get back using the `len()` function. Is it 68? You're good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Saving the data into a CSV file\n",
    "\n",
    "Now that we have a beautifully crafted URL that gives us all the data we want, let's save it in a CSV file so we can open it up in ÔΩ°ÔΩ•:*:ÔΩ•Ôæü‚òÖ,ÔΩ°ÔΩ•:*:ÔΩ•Ôæü‚òÜùî∞ùî≠ùîØùî¢ùîûùî°ùî∞ùî•ùî¢ùî¢ùî± ùî£ùî¨ùîØùî™ÔΩ°ÔΩ•:*:ÔΩ•Ôæü‚òÖ,ÔΩ°ÔΩ•:*:ÔΩ•Ôæü‚òÜ.\n",
    "\n",
    "#### Save the data to a file called `\"../output/2018_NY_mortgage_complaints.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Collect mortgage complaints from multiple states!\n",
    "**For an extra point:** write a script that loops through the list of states below, downloads all complaints between January 1, 2018 and January 1, 2019 that are about the sub-product \"Mortgage debt\", and save each into their own csv, that has the filename format `../output/2018_STATENAME_mortgage_complaints.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['NY', 'NJ', 'NV', 'ND', 'NM', 'NC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Scraping Microworkers.com\n",
    "\n",
    "For Part Two, you will be scraping an archive I've made of [Microworkers](https://www.microworkers.com/), a site that pays small amounts of money for the completion of short tasks. I have archived their \"Twitter\" job listings.\n",
    "\n",
    "You will have to:\n",
    "1. Scrape the homepage for links to each job listing\n",
    "2. Figure out how to scrape a single job listing\n",
    "3. Apply the knowledge you learned from **(2)** to each link from **(1)**\n",
    "\n",
    "The link to the archive is here:<br>\n",
    "**[http://maddy.zone/microworker/index.html](http://maddy.zone/microworker/index.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping the homepage\n",
    "\n",
    "#### Open the homepage using Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= \"http://maddy.zone/microworker/index.html\"\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the page using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate each job listing url and add them to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "main_box = soup.find(\"div\", class_=\"joblistarea\")\n",
    "#print(main_box)\n",
    "for i in main_box: \n",
    "    link = main_box.find('a')['href']\n",
    "    #print(type(link))\n",
    "    job_link = \"http://maddy.zone/microworker\" + link\n",
    "    result.append(job_link)\n",
    "    #print(job_link)\n",
    "    #print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scraping a single job listing\n",
    "\n",
    "![screenshot of the linked page](example.png)\n",
    "\n",
    "For each page, we will collect **five** different pieces of information:\n",
    "1. Job title\n",
    "2. Job ID\n",
    "3. Employer ID\n",
    "4. Payment\n",
    "5. Description\n",
    "\n",
    "But scraping them all at once can be overwhelming! Let's scrape a signle listing first. For some of the pieces of information, you might want to look into `.replace()` and `.strip()` functions for strings.\n",
    "\n",
    "#### Open `http://maddy.zone/microworker/54y2h5e4j5c4z213o503w2b4.html` using Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://maddy.zone/microworker/54y2h5e4j5c4z213o503w2b4.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the page using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate the job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE Shaw Twitter: Follow + Retweet\n"
     ]
    }
   ],
   "source": [
    "title = soup.find('div', class_='jobarealeft')\n",
    "job_title = title.find('h1').text.strip()\n",
    "print(job_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate the job id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID:\n",
      "            b1befe34f477\n"
     ]
    }
   ],
   "source": [
    "Id = soup.find('div', class_= 'jobdetailsnoteleft')\n",
    "job_id = Id.find('p', text = re.compile('Job ID')).text.strip()\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate the employer id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Member_1014973\n"
     ]
    }
   ],
   "source": [
    "E_Id = soup.find('div', class_= 'jobdetailsnoteright')\n",
    "employer_id = E_Id.find('a').text\n",
    "print(employer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate the payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will earn\n",
      "            $0.75\n"
     ]
    }
   ],
   "source": [
    "pay = soup.find('div', class_= 'jobdetailsnoteleft')\n",
    "payment = pay.find_all('p')[1].text.strip()\n",
    "print(payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is expected from Workers?\n",
      "\n",
      "1. Go to this link - https://twitter.com/DEShawInsider/status/1176597146776289281\n",
      "2. Follow this account on Twitter\n",
      "3. Retweet this recent post\n",
      "4. Take a screenshot of the repost\n",
      "\n",
      "\n",
      "\n",
      "            Required proof that task was finished?\n",
      "\n",
      "1. Take a screenshot of the repost\n"
     ]
    }
   ],
   "source": [
    "description = soup.find('div', class_= 'jobdetailsbox').text.strip()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store each of your variables into this dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_listing = {\n",
    "    'job_title': job_title        ,\n",
    "    'job_id': job_id           ,\n",
    "    'employer_id': employer_id      , \n",
    "    'payment': payment          , \n",
    "    'description':  description     ,\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scraping all of the linked pages\n",
    "\n",
    "#### Make an empty array for your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through each of the listing links that you saved in Step 1, and...<br>    Use the code from Step 2 to get the data from each listing page<br>And add the dictionary you make to the array from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in job_listing:\n",
    "    title = soup.find('div', class_='jobarealeft')\n",
    "    job_title = title.find('h1').text.strip()\n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving the data into a CSV file\n",
    "\n",
    "üéâ Wooo! you have all of data! \n",
    "\n",
    "#### Print each row into a spreadsheet called `\"../output/twitter_microworkers.csv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
